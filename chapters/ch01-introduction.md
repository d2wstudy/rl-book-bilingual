# Chapter 1

# Introduction

The idea that we learn by interacting with our environment is probably the first to occurto us when we think about the nature of learning. When an infant plays, waves its arms,or looks about, it has no explicit teacher, but it does have a direct sensorimotor connectionto its environment. Exercising this connection produces a wealth of information aboutcause and e↵ect, about the consequences of actions, and about what to do in order toachieve goals. Throughout our lives, such interactions are undoubtedly a major sourceof knowledge about our environment and ourselves. Whether we are learning to drivea car or to hold a conversation, we are acutely aware of how our environment respondsto what we do, and we seek to influence what happens through our behavior. Learningfrom interaction is a foundational idea underlying nearly all theories of learning andintelligence.

In this book we explore a computational approach to learning from interaction. Ratherthan directly theorizing about how people or animals learn, we primarily explore idealizedlearning situations and evaluate the e↵ectiveness of various learning methods. Thatis, we adopt the perspective of an artificial intelligence researcher or engineer. Weexplore designs for machines that are e↵ective in solving learning problems of scientific oreconomic interest, evaluating the designs through mathematical analysis or computationalexperiments. The approach we explore, called reinforcement learning, is much morefocused on goal-directed learning from interaction than are other approaches to machinelearning.

# 1.1 Reinforcement Learning

Reinforcement learning is learning what to do—how to map situations to actions—soas to maximize a numerical reward signal. The learner is not told which actions totake, but instead must discover which actions yield the most reward by trying them. Inthe most interesting and challenging cases, actions may a↵ect not only the immediatereward but also the next situation and, through that, all subsequent rewards. These twocharacteristics—trial-and-error search and delayed reward—are the two most importantdistinguishing features of reinforcement learning.

Reinforcement learning, like many topics whose names end with “ing,” such as machinelearning and mountaineering, is simultaneously a problem, a class of solution methodsthat work well on the problem, and the field that studies this problem and its solutionmethods. It is convenient to use a single name for all three things, but at the same timeessential to keep the three conceptually separate. In particular, the distinction betweenproblems and solution methods is very important in reinforcement learning; failing tomake this distinction is the source of many confusions.

We formalize the problem of reinforcement learning using ideas from dynamical sys-tems theory, specifically, as the optimal control of incompletely-known Markov decisionprocesses. The details of this formalization must wait until Chapter 3, but the basic ideais simply to capture the most important aspects of the real problem facing a learningagent interacting over time with its environment to achieve a goal. A learning agentmust be able to sense the state of its environment to some extent and must be able totake actions that a↵ect the state. The agent also must have a goal or goals relating tothe state of the environment. Markov decision processes are intended to include justthese three aspects—sensation, action, and goal—in their simplest possible forms withouttrivializing any of them. Any method that is well suited to solving such problems weconsider to be a reinforcement learning method.

Reinforcement learning is di↵erent from supervised learning, the kind of learning studiedin most current research in the field of machine learning. Supervised learning is learningfrom a training set of labeled examples provided by a knowledgable external supervisor.Each example is a description of a situation together with a specification—the label—ofthe correct action the system should take in that situation, which is often to identify acategory to which the situation belongs. The object of this kind of learning is for thesystem to extrapolate, or generalize, its responses so that it acts correctly in situationsnot present in the training set. This is an important kind of learning, but alone it is notadequate for learning from interaction. In interactive problems it is often impractical toobtain examples of desired behavior that are both correct and representative of all thesituations in which the agent has to act. In uncharted territory—where one would expectlearning to be most beneficial—an agent must be able to learn from its own experience.

Reinforcement learning is also di↵erent from what machine learning researchers callunsupervised learning, which is typically about finding structure hidden in collections ofunlabeled data. The terms supervised learning and unsupervised learning would seemto exhaustively classify machine learning paradigms, but they do not. Although onemight be tempted to think of reinforcement learning as a kind of unsupervised learningbecause it does not rely on examples of correct behavior, reinforcement learning is tryingto maximize a reward signal instead of trying to find hidden structure. Uncoveringstructure in an agent’s experience can certainly be useful in reinforcement learning, but byitself does not address the reinforcement learning problem of maximizing a reward signal.We therefore consider reinforcement learning to be a third machine learning paradigm,alongside supervised learning and unsupervised learning and perhaps other paradigms.

One of the challenges that arise in reinforcement learning, and not in other kindsof learning, is the trade-o↵ between exploration and exploitation. To obtain a lot ofreward, a reinforcement learning agent must prefer actions that it has tried in the pastand found to be e↵ective in producing reward. But to discover such actions, it has totry actions that it has not selected before. The agent has to exploit what it has alreadyexperienced in order to obtain reward, but it also has to explore in order to make betteraction selections in the future. The dilemma is that neither exploration nor exploitationcan be pursued exclusively without failing at the task. The agent must try a variety ofactions and progressively favor those that appear to be best. On a stochastic task, eachaction must be tried many times to gain a reliable estimate of its expected reward. Theexploration–exploitation dilemma has been intensively studied by mathematicians formany decades, yet remains unresolved. For now, we simply note that the entire issue ofbalancing exploration and exploitation does not even arise in supervised and unsupervisedlearning, at least in the purest forms of these paradigms.

Another key feature of reinforcement learning is that it explicitly considers the wholeproblem of a goal-directed agent interacting with an uncertain environment. This is incontrast to many approaches that consider subproblems without addressing how theymight fit into a larger picture. For example, we have mentioned that many machinelearning researchers have studied supervised learning without specifying how such anability would ultimately be useful. Other researchers have developed theories of planningwith general goals, but without considering planning’s role in real-time decision making,or the question of where the predictive models necessary for planning would come from.Although these approaches have yielded many useful results, their focus on isolatedsubproblems is a significant limitation.

Reinforcement learning takes the opposite tack, starting with a complete, interactive,goal-seeking agent. All reinforcement learning agents have explicit goals, can senseaspects of their environments, and can choose actions to influence their environments.Moreover, it is usually assumed from the beginning that the agent has to operate despitesignificant uncertainty about the environment it faces. When reinforcement learninginvolves planning, it has to address the interplay between planning and real-time actionselection, as well as the question of how environment models are acquired and improved.When reinforcement learning involves supervised learning, it does so for specific reasonsthat determine which capabilities are critical and which are not. For learning research tomake progress, important subproblems have to be isolated and studied, but they shouldbe subproblems that play clear roles in complete, interactive, goal-seeking agents, even ifall the details of the complete agent cannot yet be filled in.

By a complete, interactive, goal-seeking agent we do not always mean something likea complete organism or robot. These are clearly examples, but a complete, interactive,goal-seeking agent can also be a component of a larger behaving system. In this case, theagent directly interacts with the rest of the larger system and indirectly interacts withthe larger system’s environment. A simple example is an agent that monitors the chargelevel of robot’s battery and sends commands to the robot’s control architecture. Thisagent’s environment is the rest of the robot together with the robot’s environment. It is

important to look beyond the most obvious examples of agents and their environmentsto appreciate the generality of the reinforcement learning framework.

One of the most exciting aspects of modern reinforcement learning is its substantiveand fruitful interactions with other engineering and scientific disciplines. Reinforcementlearning is part of a decades-long trend within artificial intelligence and machine learningtoward greater integration with statistics, optimization, and other mathematical subjects.For example, the ability of some reinforcement learning methods to learn with parameter-ized approximators addresses the classical “curse of dimensionality” in operations researchand control theory. More distinctively, reinforcement learning has also interacted stronglywith psychology and neuroscience, with substantial benefits going both ways. Of all theforms of machine learning, reinforcement learning is the closest to the kind of learningthat humans and other animals do, and many of the core algorithms of reinforcementlearning were originally inspired by biological learning systems. Reinforcement learninghas also given back, both through a psychological model of animal learning that bettermatches some of the empirical data, and through an influential model of parts of thebrain’s reward system. The body of this book develops the ideas of reinforcement learningthat pertain to engineering and artificial intelligence, with connections to psychology andneuroscience summarized in Chapters 14 and 15.

Finally, reinforcement learning is also part of a larger trend in artificial intelligenceback toward simple general principles. Since the late 1960s, many artificial intelligence re-searchers presumed that there are no general principles to be discovered, that intelligence isinstead due to the possession of a vast number of special purpose tricks, procedures, andheuristics. It was sometimes said that if we could just get enough relevant facts into amachine, say one million, or one billion, then it would become intelligent. Methods basedon general principles, such as search or learning, were characterized as “weak methods,”whereas those based on specific knowledge were called “strong methods.” This view isuncommon today. From our point of view, it was premature: too little e↵ort had beenput into the search for general principles to conclude that there were none. Modernartificial intelligence now includes much research looking for general principles of learning,search, and decision making. It is not clear how far back the pendulum will swing, butreinforcement learning research is certainly part of the swing back toward simpler andfewer general principles of artificial intelligence.

# 1.2 Examples

A good way to understand reinforcement learning is to consider some of the examplesand possible applications that have guided its development.

• A master chess player makes a move. The choice is informed both by planning—anticipating possible replies and counterreplies—and by immediate, intuitive judg-ments of the desirability of particular positions and moves.

• An adaptive controller adjusts parameters of a petroleum refinery’s operation inreal time. The controller optimizes the yield/cost/quality trade-o↵ on the basis

of specified marginal costs without sticking strictly to the set points originallysuggested by engineers.

• A gazelle calf struggles to its feet minutes after being born. Half an hour later it isrunning at 20 miles per hour.

• A mobile robot decides whether it should enter a new room in search of more trashto collect or start trying to find its way back to its battery recharging station. Itmakes its decision based on the current charge level of its battery and how quicklyand easily it has been able to find the recharger in the past.

• Phil prepares his breakfast. Closely examined, even this apparently mundaneactivity reveals a complex web of conditional behavior and interlocking goal–subgoalrelationships: walking to the cupboard, opening it, selecting a cereal box, thenreaching for, grasping, and retrieving the box. Other complex, tuned, interactivesequences of behavior are required to obtain a bowl, spoon, and milk carton. Eachstep involves a series of eye movements to obtain information and to guide reachingand locomotion. Rapid judgments are continually made about how to carry theobjects or whether it is better to ferry some of them to the dining table beforeobtaining others. Each step is guided by goals, such as grasping a spoon or gettingto the refrigerator, and is in service of other goals, such as having the spoon to eatwith once the cereal is prepared and ultimately obtaining nourishment. Whetherhe is aware of it or not, Phil is accessing information about the state of his bodythat determines his nutritional needs, level of hunger, and food preferences.

These examples share features that are so basic that they are easy to overlook. Allinvolve interaction between an active decision-making agent and its environment, withinwhich the agent seeks to achieve a goal despite uncertainty about its environment. Theagent’s actions are permitted to a↵ect the future state of the environment (e.g., thenext chess position, the level of reservoirs of the refinery, the robot’s next location andthe future charge level of its battery), thereby a↵ecting the actions and opportunitiesavailable to the agent at later times. Correct choice requires taking into account indirect,delayed consequences of actions, and thus may require foresight or planning.

At the same time, in all of these examples the e↵ects of actions cannot be fully predicted;thus the agent must monitor its environment frequently and react appropriately. Forexample, Phil must watch the milk he pours into his cereal bowl to keep it from overflowing.All these examples involve goals that are explicit in the sense that the agent can judgeprogress toward its goal based on what it can sense directly. The chess player knowswhether or not he wins, the refinery controller knows how much petroleum is beingproduced, the gazelle calf knows when it falls, the mobile robot knows when its batteriesrun down, and Phil knows whether or not he is enjoying his breakfast.

In all of these examples the agent can use its experience to improve its performanceover time. The chess player refines the intuition he uses to evaluate positions, therebyimproving his play; the gazelle calf improves the e ciency with which it can run; Phillearns to streamline making his breakfast. The knowledge the agent brings to the task atthe start—either from previous experience with related tasks or built into it by design or

evolution—influences what is useful or easy to learn, but interaction with the environmentis essential for adjusting behavior to exploit specific features of the task.

# 1.3 Elements of Reinforcement Learning

Beyond the agent and the environment, one can identify four main subelements of areinforcement learning system: a policy, a reward signal, a value function, and, optionally,a model of the environment.

A policy defines the learning agent’s way of behaving at a given time. Roughly speaking,a policy is a mapping from perceived states of the environment to actions to be takenwhen in those states. It corresponds to what in psychology would be called a set ofstimulus–response rules or associations. In some cases the policy may be a simple functionor lookup table, whereas in others it may involve extensive computation such as a searchprocess. The policy is the core of a reinforcement learning agent in the sense that it aloneis su cient to determine behavior. In general, policies may be stochastic, specifyingprobabilities for each action.

A reward signal defines the goal of a reinforcement learning problem. On each timestep, the environment sends to the reinforcement learning agent a single number calledthe reward. The agent’s sole objective is to maximize the total reward it receives overthe long run. The reward signal thus defines what are the good and bad events for theagent. In a biological system, we might think of rewards as analogous to the experiencesof pleasure or pain. They are the immediate and defining features of the problem facedby the agent. The reward signal is the primary basis for altering the policy; if an actionselected by the policy is followed by low reward, then the policy may be changed toselect some other action in that situation in the future. In general, reward signals maybe stochastic functions of the state of the environment and the actions taken.

Whereas the reward signal indicates what is good in an immediate sense, a valuefunction specifies what is good in the long run. Roughly speaking, the value of a state isthe total amount of reward an agent can expect to accumulate over the future, startingfrom that state. Whereas rewards determine the immediate, intrinsic desirability ofenvironmental states, values indicate the long-term desirability of states after taking intoaccount the states that are likely to follow and the rewards available in those states. Forexample, a state might always yield a low immediate reward but still have a high valuebecause it is regularly followed by other states that yield high rewards. Or the reversecould be true. To make a human analogy, rewards are somewhat like pleasure (if high)and pain (if low), whereas values correspond to a more refined and farsighted judgmentof how pleased or displeased we are that our environment is in a particular state.

Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary.Without rewards there could be no values, and the only purpose of estimating values is toachieve more reward. Nevertheless, it is values with which we are most concerned whenmaking and evaluating decisions. Action choices are made based on value judgments. Weseek actions that bring about states of highest value, not highest reward, because theseactions obtain the greatest amount of reward for us over the long run. Unfortunately, itis much harder to determine values than it is to determine rewards. Rewards are basicallygiven directly by the environment, but values must be estimated and re-estimated from

the sequences of observations an agent makes over its entire lifetime. In fact, the mostimportant component of almost all reinforcement learning algorithms we consider is amethod for e ciently estimating values. The central role of value estimation is arguablythe most important thing that has been learned about reinforcement learning over thelast six decades.

The fourth and final element of some reinforcement learning systems is a model ofthe environment. This is something that mimics the behavior of the environment, ormore generally, that allows inferences to be made about how the environment will behave.For example, given a state and action, the model might predict the resultant next stateand next reward. Models are used for planning, by which we mean any way of decidingon a course of action by considering possible future situations before they are actuallyexperienced. Methods for solving reinforcement learning problems that use models andplanning are called model-based methods, as opposed to simpler model-free methods thatare explicitly trial-and-error learners—viewed as almost the opposite of planning. InChapter 8 we explore reinforcement learning systems that simultaneously learn by trialand error, learn a model of the environment, and use the model for planning. Modernreinforcement learning spans the spectrum from low-level, trial-and-error learning tohigh-level, deliberative planning.

# 1.4 Limitations and Scope

Reinforcement learning relies heavily on the concept of state—as input to the policy andvalue function, and as both input to and output from the model. Informally, we canthink of the state as a signal conveying to the agent some sense of “how the environmentis” at a particular time. The formal definition of state as we use it here is given bythe framework of Markov decision processes presented in Chapter 3. More generally,however, we encourage the reader to follow the informal meaning and think of the stateas whatever information is available to the agent about its environment. In e↵ect, weassume that the state signal is produced by some preprocessing system that is nominallypart of the agent’s environment. We do not address the issues of constructing, changing,or learning the state signal in this book (other than briefly in Section 17.3). We take thisapproach not because we consider state representation to be unimportant, but in orderto focus fully on the decision-making issues. In other words, our concern in this book isnot with designing the state signal, but with deciding what action to take as a functionof whatever state signal is available.

Most of the reinforcement learning methods we consider in this book are structuredaround estimating value functions, but it is not strictly necessary to do this to solvereinforcement learning problems. For example, solution methods such as genetic algo-rithms, genetic programming, simulated annealing, and other optimization methods neverestimate value functions. These methods apply multiple static policies each interactingover an extended period of time with a separate instance of the environment. The policiesthat obtain the most reward, and random variations of them, are carried over to thenext generation of policies, and the process repeats. We call these evolutionary methodsbecause their operation is analogous to the way biological evolution produces organisms

with skilled behavior even if they do not learn during their individual lifetimes. If thespace of policies is su ciently small, or can be structured so that good policies arecommon or easy to find—or if a lot of time is available for the search—then evolutionarymethods can be e↵ective. In addition, evolutionary methods have advantages on problemsin which the learning agent cannot sense the complete state of its environment.

Our focus is on reinforcement learning methods that learn while interacting with theenvironment, which evolutionary methods do not do. Methods able to take advantageof the details of individual behavioral interactions can be much more e cient thanevolutionary methods in many cases. Evolutionary methods ignore much of the usefulstructure of the reinforcement learning problem: they do not use the fact that the policythey are searching for is a function from states to actions; they do not notice which statesan individual passes through during its lifetime, or which actions it selects. In some casessuch information can be misleading (e.g., when states are misperceived), but more often itshould enable more e cient search. Although evolution and learning share many featuresand naturally work together, we do not consider evolutionary methods by themselves tobe especially well suited to reinforcement learning problems and, accordingly, we do notcover them in this book.

# 1.5 An Extended Example: Tic-Tac-Toe

To illustrate the general idea of reinforcement learning and contrast it with other ap-proaches, we next consider a single example in more detail.

Consider the familiar child’s game of tic-tac-toe. Two playerstake turns playing on a three-by-three board. One player playsXs and the other Os until one player wins by placing three marksin a row, horizontally, vertically, or diagonally, as the X playerhas in the game shown to the right. If the board fills up withneither player getting three in a row, then the game is a draw.Because a skilled player can play so as never to lose, let us assumethat we are playing against an imperfect player, one whose playis sometimes incorrect and allows us to win. For the moment, in

fact, let us consider draws and losses to be equally bad for us. How might we construct aplayer that will find the imperfections in its opponent’s play and learn to maximize itschances of winning?

Although this is a simple problem, it cannot readily be solved in a satisfactory waythrough classical techniques. For example, the classical “minimax” solution from gametheory is not correct here because it assumes a particular way of playing by the opponent.For example, a minimax player would never reach a game state from which it couldlose, even if in fact it always won from that state because of incorrect play by theopponent. Classical optimization methods for sequential decision problems, such asdynamic programming, can compute an optimal solution for any opponent, but requireas input a complete specification of that opponent, including the probabilities with whichthe opponent makes each move in each board state. Let us assume that this informationis not available a priori for this problem, as it is not for the vast majority of problems of

![image](https://cdn-mineru.openxlab.org.cn/result/2026-02-07/86bd4ca5-f410-4074-81e6-d775988a05ab/a519470e001d36e4e7c0bbddadc8ba815e1aae1000e12aacf0027cd9a4ef5f05.jpg)


practical interest. On the other hand, such information can be estimated from experience,in this case by playing many games against the opponent. About the best one can doon this problem is first to learn a model of the opponent’s behavior, up to some level ofconfidence, and then apply dynamic programming to compute an optimal solution giventhe approximate opponent model. In the end, this is not that di↵erent from some of thereinforcement learning methods we examine later in this book.

An evolutionary method applied to this problem would directly search the spaceof possible policies for one with a high probability of winning against the opponent.Here, a policy is a rule that tells the player what move to make for every state of thegame—every possible configuration of Xs and Os on the three-by-three board. For eachpolicy considered, an estimate of its winning probability would be obtained by playingsome number of games against the opponent. This evaluation would then direct whichpolicy or policies were considered next. A typical evolutionary method would hill-climbin policy space, successively generating and evaluating policies in an attempt to obtainincremental improvements. Or, perhaps, a genetic-style algorithm could be used thatwould maintain and evaluate a population of policies. Literally hundreds of di↵erentoptimization methods could be applied.

Here is how the tic-tac-toe problem would be approached with a method making useof a value function. First we would set up a table of numbers, one for each possible stateof the game. Each number will be the latest estimate of the probability of our winningfrom that state. We treat this estimate as the state’s value, and the whole table is thelearned value function. State A has higher value than state B, or is considered “better”than state B, if the current estimate of the probability of our winning from A is higherthan it is from B. Assuming we always play Xs, then for all states with three Xs in a rowthe probability of winning is 1, because we have already won. Similarly, for all stateswith three Os in a row, or that are filled up, the correct probability is 0, as we cannotwin from them. We set the initial values of all the other states to 0.5, representing aguess that we have a $5 0 \%$ chance of winning.

We then play many games against the opponent. To select our moves we examine thestates that would result from each of our possible moves (one for each blank space on theboard) and look up their current values in the table. Most of the time we move greedily,selecting the move that leads to the state with greatest value, that is, with the highestestimated probability of winning. Occasionally, however, we select randomly from amongthe other moves instead. These are called exploratory moves because they cause us toexperience states that we might otherwise never see. A sequence of moves made andconsidered during a game can be diagrammed as in Figure 1.1.

While we are playing, we change the values of the states in which we find ourselvesduring the game. We attempt to make them more accurate estimates of the probabilitiesof winning. To do this, we “back up” the value of the state after each greedy move tothe state before the move, as suggested by the arrows in Figure 1.1. More precisely, thecurrent value of the earlier state is updated to be closer to the value of the later state.This can be done by moving the earlier state’s value a fraction of the way toward thevalue of the later state. If we let $S _ { t }$ denote the state before the greedy move, and $S _ { t + 1 }$the state after that move, then the update to the estimated value of $S _ { t }$ , denoted $V ( S _ { t } )$

![image](https://cdn-mineru.openxlab.org.cn/result/2026-02-07/86bd4ca5-f410-4074-81e6-d775988a05ab/8bee8e311bb934162e99d6f9e493a74266d44988bba0bf89e1fa41785fbf1628.jpg)



Figure 1.1: A sequence of tic-tac-toe moves. The solid black lines represent the moves takenduring a game; the dashed lines represent moves that we (our reinforcement learning player)considered but did not make. The * indicates the move currently estimated to be the best. Oursecond move was an exploratory move, meaning that it was taken even though another siblingmove, the one leading to $\mathrm { e ^ { \ast } }$ , was ranked higher. Exploratory moves do not result in any learning,but each of our other moves does, causing updates as suggested by the red arrows in whichestimated values are moved up the tree from later nodes to earlier nodes as detailed in the text.


can be written as

$$
V (S _ {t}) \leftarrow V (S _ {t}) + \alpha \Big [ V (S _ {t + 1}) - V (S _ {t}) \Big ],
$$

where $\alpha$ is a small positive fraction called the step-size parameter, which influencesthe rate of learning. This update rule is an example of a temporal-di↵erence learningmethod, so called because its changes are based on a di↵erence, $V ( S _ { t + 1 } ) - V ( S _ { t } )$ , betweenestimates at two successive times.

The method described above performs quite well on this task. For example, if thestep-size parameter is reduced properly over time, then this method converges, for anyfixed opponent, to the true probabilities of winning from each state given optimal playby our player. Furthermore, the moves then taken (except on exploratory moves) are infact the optimal moves against this (imperfect) opponent. In other words, the methodconverges to an optimal policy for playing the game against this opponent. If the step-sizeparameter is not reduced all the way to zero over time, then this player also plays wellagainst opponents that slowly change their way of playing.

This example illustrates the di↵erences between evolutionary methods and methodsthat learn value functions. To evaluate a policy, an evolutionary method holds the policyfixed and plays many games against the opponent or simulates many games using a modelof the opponent. The frequency of wins gives an unbiased estimate of the probabilityof winning with that policy, and can be used to direct the next policy selection. Buteach policy change is made only after many games, and only the final outcome of eachgame is used: what happens during the games is ignored. For example, if the player wins,then all of its behavior in the game is given credit, independently of how specific movesmight have been critical to the win. Credit is even given to moves that never occurred!Value function methods, in contrast, allow individual states to be evaluated. In the end,evolutionary and value function methods both search the space of policies, but learning avalue function takes advantage of information available during the course of play.

This simple example illustrates some of the key features of reinforcement learningmethods. First, there is the emphasis on learning while interacting with an environment,in this case with an opponent player. Second, there is a clear goal, and correct behaviorrequires planning or foresight that takes into account delayed e↵ects of one’s choices. Forexample, the simple reinforcement learning player would learn to set up multi-move trapsfor a shortsighted opponent. It is a striking feature of the reinforcement learning solutionthat it can achieve the e↵ects of planning and lookahead without using a model of theopponent and without conducting an explicit search over possible sequences of futurestates and actions.

While this example illustrates some of the key features of reinforcement learning, it isso simple that it might give the impression that reinforcement learning is more limitedthan it really is. Although tic-tac-toe is a two-person game, reinforcement learningalso applies in the case in which there is no external adversary, that is, in the case ofa “game against nature.” Reinforcement learning also is not restricted to problems inwhich behavior breaks down into separate episodes, like the separate games of tic-tac-toe,with reward only at the end of each episode. It is just as applicable when behaviorcontinues indefinitely and when rewards of various magnitudes can be received at anytime. Reinforcement learning is also applicable to problems that do not even break downinto discrete time steps like the plays of tic-tac-toe. The general principles apply tocontinuous-time problems as well, although the theory gets more complicated and weomit it from this introductory treatment.

Tic-tac-toe has a relatively small, finite state set, whereas reinforcement learning canbe used when the state set is very large, or even infinite. For example, Gerry Tesauro(1992, 1995) combined the algorithm described above with an artificial neural network tolearn to play backgammon, which has approximately $1 0 ^ { 2 0 }$ states. With this many statesit is impossible ever to experience more than a small fraction of them. Tesauro’s programlearned to play far better than any previous program and eventually better than theworld’s best human players (see Section 16.1). The artificial neural network provides theprogram with the ability to generalize from its experience, so that in new states it selectsmoves based on information saved from similar states faced in the past, as determinedby the network. How well a reinforcement learning system can work in problems withsuch large state sets is intimately tied to how appropriately it can generalize from past

experience. It is in this role that we have the greatest need for supervised learningmethods within reinforcement learning. Artificial neural networks and deep learning(Section 9.7) are not the only, or necessarily the best, way to do this.

In this tic-tac-toe example, learning started with no prior knowledge beyond therules of the game, but reinforcement learning by no means entails a tabula rasa view oflearning and intelligence. On the contrary, prior information can be incorporated intoreinforcement learning in a variety of ways that can be critical for e cient learning (e.g.,see Sections 9.5, 17.4, and 13.1). We also have access to the true state in the tic-tac-toeexample, whereas reinforcement learning can also be applied when part of the state ishidden, or when di↵erent states appear to the learner to be the same.

Finally, the tic-tac-toe player was able to look ahead and know the states that wouldresult from each of its possible moves. To do this, it had to have a model of the gamethat allowed it to foresee how its environment would change in response to moves that itmight never make. Many problems are like this, but in others even a short-term modelof the e↵ects of actions is lacking. Reinforcement learning can be applied in either case.A model is not required, but models can easily be used if they are available or can belearned (Chapter 8).

On the other hand, there are reinforcement learning methods that do not need anykind of environment model at all. Model-free systems cannot even think about howtheir environments will change in response to a single action. The tic-tac-toe player ismodel-free in this sense with respect to its opponent: it has no model of its opponentof any kind. Because models have to be reasonably accurate to be useful, model-freemethods can have advantages over more complex methods when the real bottleneck insolving a problem is the di culty of constructing a su ciently accurate environmentmodel. Model-free methods are also important building blocks for model-based methods.In this book we devote several chapters to model-free methods before we discuss howthey can be used as components of more complex model-based methods.

Reinforcement learning can be used at both high and low levels in a system. Althoughthe tic-tac-toe player learned only about the basic moves of the game, nothing preventsreinforcement learning from working at higher levels where each of the “actions” mayitself be the application of a possibly elaborate problem-solving method. In hierarchicallearning systems, reinforcement learning can work simultaneously on several levels.

Exercise 1.1: Self-Play Suppose, instead of playing against a random opponent, thereinforcement learning algorithm described above played against itself, with both sideslearning. What do you think would happen in this case? Would it learn a di↵erent policyfor selecting moves? ⇤

Exercise 1.2: Symmetries Many tic-tac-toe positions appear di↵erent but are reallythe same because of symmetries. How might we amend the learning process describedabove to take advantage of this? In what ways would this change improve the learningprocess? Now think again. Suppose the opponent did not take advantage of symmetries.In that case, should we? Is it true, then, that symmetrically equivalent positions shouldnecessarily have the same value? ⇤

Exercise 1.3: Greedy Play Suppose the reinforcement learning player was greedy, that is,it always played the move that brought it to the position that it rated the best. Might it

learn to play better, or worse, than a nongreedy player? What problems might occur? ⇤

Exercise 1.4: Learning from Exploration Suppose learning updates occurred after allmoves, including exploratory moves. If the step-size parameter is appropriately reducedover time (but not the tendency to explore), then the state values would converge toa di↵erent set of probabilities. What (conceptually) are the two sets of probabilitiescomputed when we do, and when we do not, learn from exploratory moves? Assumingthat we do continue to make exploratory moves, which set of probabilities might be betterto learn? Which would result in more wins? ⇤

Exercise 1.5: Other Improvements Can you think of other ways to improve the reinforce-ment learning player? Can you think of any better way to solve the tic-tac-toe problemas posed? ⇤

# 1.6 Summary

Reinforcement learning is a computational approach to understanding and automatinggoal-directed learning and decision making. It is distinguished from other computationalapproaches by its emphasis on learning by an agent from direct interaction with itsenvironment, without requiring exemplary supervision or complete models of the envi-ronment. In our opinion, reinforcement learning is the first field to seriously address thecomputational issues that arise when learning from interaction with an environment inorder to achieve long-term goals.

Reinforcement learning uses the formal framework of Markov decision processes todefine the interaction between a learning agent and its environment in terms of states,actions, and rewards. This framework is intended to be a simple way of representingessential features of the artificial intelligence problem. These features include a sense ofcause and e↵ect, a sense of uncertainty and nondeterminism, and the existence of explicitgoals.

The concepts of value and value function are key to most of the reinforcement learningmethods that we consider in this book. We take the position that value functionsare important for e cient search in the space of policies. The use of value functionsdistinguishes reinforcement learning methods from evolutionary methods that searchdirectly in policy space guided by evaluations of entire policies.

# 1.7 Early History of Reinforcement Learning

The early history of reinforcement learning has two main threads, both long and rich, thatwere pursued independently before intertwining in modern reinforcement learning. Onethread concerns learning by trial and error, and originated in the psychology of animallearning. This thread runs through some of the earliest work in artificial intelligenceand led to the revival of reinforcement learning in the early 1980s. The second threadconcerns the problem of optimal control and its solution using value functions anddynamic programming. For the most part, this thread did not involve learning. Thetwo threads were mostly independent, but became interrelated to some extent around a

third, less distinct thread concerning temporal-di↵erence methods such as that used inthe tic-tac-toe example in this chapter. All three threads came together in the late 1980sto produce the modern field of reinforcement learning as we present it in this book.

The thread focusing on trial-and-error learning is the one with which we are mostfamiliar and about which we have the most to say in this brief history. Before doing that,however, we briefly discuss the optimal control thread.

The term “optimal control” came into use in the late 1950s to describe the problem ofdesigning a controller to minimize or maximize a measure of a dynamical system’s behaviorover time. One of the approaches to this problem was developed in the mid-1950s byRichard Bellman and others through extending a nineteenth century theory of Hamiltonand Jacobi. This approach uses the concepts of a dynamical system’s state and of avalue function, or “optimal return function,” to define a functional equation, now oftencalled the Bellman equation. The class of methods for solving optimal control problemsby solving this equation came to be known as dynamic programming (Bellman, 1957a).Bellman (1957b) also introduced the discrete stochastic version of the optimal controlproblem known as Markov decision processes (MDPs). Ronald Howard (1960) devisedthe policy iteration method for MDPs. All of these are essential elements underlying thetheory and algorithms of modern reinforcement learning.

Dynamic programming is widely considered the only feasible way of solving generalstochastic optimal control problems. It su↵ers from what Bellman called “the curse ofdimensionality,” meaning that its computational requirements grow exponentially withthe number of state variables, but it is still far more e cient and more widely applicablethan any other general method. Dynamic programming has been extensively developedsince the late 1950s, including extensions to partially observable MDPs (surveyed byLovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approximationmethods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982, 1983).Many excellent modern treatments of dynamic programming are available (e.g., Bertsekas,2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983). Bryson (1996) providesan authoritative history of optimal control.

Connections between optimal control and dynamic programming, on the one hand,and learning, on the other, were slow to be recognized. We cannot be sure about whataccounted for this separation, but its main cause was likely the separation betweenthe disciplines involved and their di↵erent goals. Also contributing may have been theprevalent view of dynamic programming as an o↵-line computation depending essentiallyon accurate system models and analytic solutions to the Bellman equation. Further,the simplest form of dynamic programming is a computation that proceeds backwardsin time, making it di cult to see how it could be involved in a learning process thatmust proceed in a forward direction. Some of the earliest work in dynamic programming,such as that by Bellman and Dreyfus (1959), might now be classified as followinga learning approach. Witten’s (1977) work (discussed below) certainly qualifies as acombination of learning and dynamic-programming ideas. Werbos (1987) argued explicitlyfor greater interrelation of dynamic programming and learning methods and for dynamicprogramming’s relevance to understanding neural and cognitive mechanisms. For us thefull integration of dynamic programming methods with online learning did not occur

until the work of Chris Watkins in 1989, whose treatment of reinforcement learning usingthe MDP formalism has been widely adopted. Since then these relationships have beenextensively developed by many researchers, most particularly by Dimitri Bertsekas andJohn Tsitsiklis (1996), who coined the term “neurodynamic programming” to refer tothe combination of dynamic programming and artificial neural networks. Another termcurrently in use is “approximate dynamic programming.” These various approachesemphasize di↵erent aspects of the subject, but they all share with reinforcement learningan interest in circumventing the classical shortcomings of dynamic programming.

We consider all of the work in optimal control also to be, in a sense, work in reinforce-ment learning. We define a reinforcement learning method as any e↵ective way of solvingreinforcement learning problems, and it is now clear that these problems are closelyrelated to optimal control problems, particularly stochastic optimal control problemssuch as those formulated as MDPs. Accordingly, we must consider the solution methodsof optimal control, such as dynamic programming, also to be reinforcement learningmethods. Because almost all of the conventional methods require complete knowledgeof the system to be controlled, it feels a little unnatural to say that they are part ofreinforcement learning. On the other hand, many dynamic programming algorithms areincremental and iterative. Like learning methods, they gradually reach the correct answerthrough successive approximations. As we show in the rest of this book, these similaritiesare far more than superficial. The theories and solution methods for the cases of completeand incomplete knowledge are so closely related that we feel they must be consideredtogether as part of the same subject matter.

Let us return now to the other major thread leading to the modern field of reinforcementlearning, the thread centered on the idea of trial-and-error learning. We only touch onthe major points of contact here, taking up this topic in more detail in Section 14.3.According to American psychologist R. S. Woodworth (1938) the idea of trial-and-errorlearning goes as far back as the 1850s to Alexander Bain’s discussion of learning by“groping and experiment” and more explicitly to the British ethologist and psychologistConway Lloyd Morgan’s 1894 use of the term to describe his observations of animalbehavior. Perhaps the first to succinctly express the essence of trial-and-error learning asa principle of learning was Edward Thorndike:

Of several responses made to the same situation, those which are accompaniedor closely followed by satisfaction to the animal will, other things beingequal, be more firmly connected with the situation, so that, when it recurs,they will be more likely to recur; those which are accompanied or closelyfollowed by discomfort to the animal will, other things being equal, have theirconnections with that situation weakened, so that, when it recurs, they willbe less likely to occur. The greater the satisfaction or discomfort, the greaterthe strengthening or weakening of the bond. (Thorndike, 1911, p. 244)

Thorndike called this the “Law of E↵ect” because it describes the e↵ect of reinforcingevents on the tendency to select actions. Thorndike later modified the law to betteraccount for subsequent data on animal learning (such as di↵erences between the e↵ectsof reward and punishment), and the law in its various forms has generated considerablecontroversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble,

1961, 1967; Mazur, 1994). Despite this, the Law of E↵ect—in one form or another—iswidely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower,1975; Dennett, 1978; Campbell, 1960; Cziko, 1995). It is the basis of the influentiallearning theories of Clark Hull (1943, 1952) and the influential experimental methods ofB. F. Skinner (1938).

The term “reinforcement” in the context of animal learning came into use well afterThorndike’s expression of the Law of E↵ect, first appearing in this context (to the best ofour knowledge) in the 1927 English translation of Pavlov’s monograph on conditionedreflexes. Pavlov described reinforcement as the strengthening of a pattern of behavior dueto an animal receiving a stimulus—a reinforcer—in an appropriate temporal relationshipwith another stimulus or with a response. Some psychologists extended the idea ofreinforcement to include weakening as well as strengthening of behavior, and extendedthe idea of a reinforcer to include possibly the omission or termination of stimulus. To beconsidered a reinforcer, the strengthening or weakening must persist after the reinforceris withdrawn; a stimulus that merely attracts an animal’s attention or that energizes itsbehavior without producing lasting changes would not be considered a reinforcer.

The idea of implementing trial-and-error learning in a computer appeared among theearliest thoughts about the possibility of artificial intelligence. In a 1948 report, AlanTuring described a design for a “pleasure-pain system” that worked along the lines of theLaw of E↵ect:

When a configuration is reached for which the action is undetermined, arandom choice for the missing data is made and the appropriate entry is madein the description, tentatively, and is applied. When a pain stimulus occursall tentative entries are cancelled, and when a pleasure stimulus occurs theyare all made permanent. (Turing, 1948)

Many ingenious electro-mechanical machines were constructed that demonstrated trial-and-error learning. The earliest may have been a machine built by Thomas Ross (1933)that was able to find its way through a simple maze and remember the path throughthe settings of switches. In 1951 W. Grey Walter built a version of his “mechanicaltortoise” (Walter, 1950) capable of a simple form of learning. In 1952 Claude Shannondemonstrated a maze-running mouse named Theseus that used trial and error to findits way through a maze, with the maze itself remembering the successful directionsvia magnets and relays under its floor (see also Shannon, 1951). J. A. Deutsch (1954)described a maze-solving machine based on his behavior theory (Deutsch, 1953) thathas some properties in common with model-based reinforcement learning (Chapter 8).In his PhD dissertation, Marvin Minsky (1954) discussed computational models ofreinforcement learning and described his construction of an analog machine composed ofcomponents he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators)meant to resemble modifiable synaptic connections in the brain (Chapter 15). Theweb site cyberneticzoo.com contains a wealth of information on these and many otherelectro-mechanical learning machines.

Building electro-mechanical learning machines gave way to programming digital com-puters to perform various types of learning, some of which implemented trial-and-errorlearning. Farley and Clark (1954) described a digital simulation of a neural-network

learning machine that learned by trial and error. But their interests soon shifted fromtrial-and-error learning to generalization and pattern recognition, that is, from reinforce-ment learning to supervised learning (Clark and Farley, 1955). This began a patternof confusion about the relationship between these types of learning. Many researchersseemed to believe that they were studying reinforcement learning when they were actuallystudying supervised learning. For example, artificial neural network pioneers such asRosenblatt (1962) and Widrow and Ho↵ (1960) were clearly motivated by reinforcementlearning—they used the language of rewards and punishments—but the systems theystudied were supervised learning systems suitable for pattern recognition and perceptuallearning. Even today, some researchers and textbooks minimize or blur the distinctionbetween these types of learning. For example, some textbooks have used the term “trial-and-error” to describe artificial neural networks that learn from training examples. Thisis an understandable confusion because these networks use error information to updateconnection weights, but this misses the essential character of trial-and-error learning asselecting actions on the basis of evaluative feedback that does not rely on knowledge ofwhat the correct action should be.

Partly as a result of these confusions, research into genuine trial-and-error learningbecame rare in the 1960s and 1970s, although there were notable exceptions. In the 1960sthe terms “reinforcement” and “reinforcement learning” were used in the engineeringliterature for the first time to describe engineering uses of trial-and-error learning (e.g.,Waltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970). Particularlyinfluential was Minsky’s paper “Steps Toward Artificial Intelligence” (Minsky, 1961),which discussed several issues relevant to trial-and-error learning, including prediction,expectation, and what he called the basic credit-assignment problem for complex rein-forcement learning systems: How do you distribute credit for success among the manydecisions that may have been involved in producing it? All of the methods we discuss inthis book are, in a sense, directed toward solving this problem. Minsky’s paper is wellworth reading today.

In the next few paragraphs we discuss some of the other exceptions and partialexceptions to the relative neglect of computational and theoretical study of genuinetrial-and-error learning in the 1960s and 1970s.

One exception was the work of the New Zealand researcher John Andreae, whodeveloped a system called STeLLA that learned by trial and error in interaction withits environment. This system included an internal model of the world and, later, an“internal monologue” to deal with problems of hidden state (Andreae, 1963, 1969; Andreaeand Cashin, 1969). Andreae’s later work (1977) placed more emphasis on learningfrom a teacher, but still included learning by trial and error, with the generation ofnovel events being one of the system’s goals. A feature of this work was a “leakbackprocess,” elaborated more fully in Andreae (1998), that implemented a credit-assignmentmechanism similar to the backing-up update operations that we describe. Unfortunately,his pioneering research was not well known and did not greatly impact subsequentreinforcement learning research. Recent summaries are available (Andreae, 2017a,b).

More influential was the work of Donald Michie. In 1961 and 1963 he described asimple trial-and-error learning system for learning how to play tic-tac-toe (or naughts

and crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). Itconsisted of a matchbox for each possible game position, each matchbox containing anumber of colored beads, a di↵erent color for each possible move from that position. Bydrawing a bead at random from the matchbox corresponding to the current game position,one could determine MENACE’s move. When a game was over, beads were added toor removed from the boxes used during play to reward or punish MENACE’s decisions.Michie and Chambers (1968) described another tic-tac-toe reinforcement learner calledGLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controllercalled BOXES. They applied BOXES to the task of learning to balance a pole hinged toa movable cart on the basis of a failure signal occurring only when the pole fell or thecart reached the end of a track. This task was adapted from the earlier work of Widrowand Smith (1964), who used supervised learning methods, assuming instruction from ateacher already able to balance the pole. Michie and Chambers’s version of pole-balancingis one of the best early examples of a reinforcement learning task under conditions ofincomplete knowledge. It influenced much later work in reinforcement learning, beginningwith some of our own studies (Barto, Sutton, and Anderson, 1983; Sutton, 1984). Michie(1974) consistently emphasized trial and error and learning as essential aspects of artificialintelligence.

Widrow, Gupta, and Maitra (1973) modified the Least-Mean-Square (LMS) algorithmof Widrow and Ho↵ (1960) to produce a reinforcement learning rule that could learnfrom success and failure signals instead of from training examples. They called this formof learning “selective bootstrap adaptation” and described it as “learning with a critic”instead of “learning with a teacher.” They analyzed this rule and showed how it couldlearn to play blackjack. This was an isolated foray into reinforcement learning by Widrow,whose contributions to supervised learning were much more influential. Our use of theterm “critic” is derived from Widrow, Gupta, and Maitra’s paper. Buchanan, Mitchell,Smith, and Johnson (1978) independently used the term critic in the context of machinelearning (see also Dietterich and Buchanan, 1984), but for them a critic was an expertsystem able to do more than evaluate performance.

Research on learning automata had a more direct influence on the trial-and-errorthread leading to modern reinforcement learning research. These are methods for solvinga nonassociative, purely selectional learning problem known as the $k$ -armed bandit byanalogy to a slot machine, or “one-armed bandit,” except with $k$ levers (see Chapter 2).Learning automata are simple, low-memory machines for improving the probabilityof reward in these problems. Learning automata originated with work in the 1960sof the Russian mathematician and physicist M. L. Tsetlin and colleagues (publishedposthumously in Tsetlin, 1973) and has been extensively developed since then withinengineering (see Narendra and Thathachar, 1974, 1989). These developments included thestudy of stochastic learning automata, which are methods for updating action probabilitieson the basis of reward signals. Although not developed in the tradition of stochasticlearning automata, Harth and Tzanakou’s (1974) Alopex algorithm (for Algorithm ofpattern extraction) is a stochastic method for detecting correlations between actions andreinforcement that influenced some of our early research (Barto, Sutton, and Brouwer,1981). Stochastic learning automata were foreshadowed by earlier work in psychology,beginning with William Estes’ (1950) e↵ort toward a statistical theory of learning andfurther developed by others (e.g., Bush and Mosteller, 1955; Sternberg, 1963).

The statistical learning theories developed in psychology were adopted by researchers ineconomics, leading to a thread of research in that field devoted to reinforcement learning.This work began in 1973 with the application of Bush and Mosteller’s learning theory toa collection of classical economic models (Cross, 1973). One goal of this research was tostudy artificial agents that act more like real people than do traditional idealized economicagents (Arthur, 1991). This approach expanded to the study of reinforcement learningin the context of game theory. Reinforcement learning in economics developed largelyindependently of the early work in reinforcement learning in artificial intelligence, thoughgame theory remains a topic of interest in both fields (beyond the scope of this book).Camerer (2011) discusses the reinforcement learning tradition in economics, and Now´e,Vrancx, and De Hauwere (2012) provide an overview of the subject from the point of viewof multi-agent extensions to the approach that we introduce in this book. Reinforcementlearning in the context of game theory is a much di↵erent subject than reinforcementlearning used in programs to play tic-tac-toe, checkers, and other recreational games. See,for example, Szita (2012) for an overview of this aspect of reinforcement learning andgames.

John Holland (1975) outlined a general theory of adaptive systems based on selectionalprinciples. His early work concerned trial and error primarily in its nonassociativeform, as in evolutionary methods and the $k$ -armed bandit. In 1976 and more fully in1986, he introduced classifier systems, true reinforcement learning systems includingassociation and value functions. A key component of Holland’s classifier systems wasthe “bucket-brigade algorithm” for credit assignment, which is closely related to thetemporal di↵erence algorithm used in our tic-tac-toe example and discussed in Chapter 6.Another key component was a genetic algorithm, an evolutionary method whose role wasto evolve useful representations. Classifier systems have been extensively developed bymany researchers to form a major branch of reinforcement learning research (reviewed byUrbanowicz and Moore, 2009), but genetic algorithms—which we do not consider to bereinforcement learning systems by themselves—have received much more attention, ashave other approaches to evolutionary computation (e.g., Fogel, Owens and Walsh, 1966;Koza, 1992).

The individual most responsible for reviving the trial-and-error thread of reinforcementlearning within artificial intelligence was Harry Klopf (1972, 1975, 1982). Klopf recognizedthat essential aspects of adaptive behavior were being lost as learning researchers cameto focus almost exclusively on supervised learning. What was missing, according toKlopf, were the hedonic aspects of behavior: the drive to achieve some result from theenvironment, to control the environment toward desired ends and away from undesiredends (see Section 15.9). This is the essential idea of trial-and-error learning. Klopf’sideas were especially influential on the authors because our assessment of them (Bartoand Sutton, 1981a) led to our appreciation of the distinction between supervised andreinforcement learning, and to our eventual focus on reinforcement learning. Much ofthe early work that we and colleagues accomplished was directed toward showing thatreinforcement learning and supervised learning were indeed di↵erent (Barto, Sutton, andBrouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985). Other studiesshowed how reinforcement learning could address important problems in artificial neural

network learning, in particular, how it could produce learning algorithms for multilayernetworks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985,1986; Barto and Jordan, 1987; see Section 15.10).

We turn now to the third thread to the history of reinforcement learning, that concerningtemporal-di↵erence learning. Temporal-di↵erence learning methods are distinctive inbeing driven by the di↵erence between temporally successive estimates of the samequantity—for example, of the probability of winning in the tic-tac-toe example. Thisthread is smaller and less distinct than the other two, but it has played a particularlyimportant role in the field, in part because temporal-di↵erence methods seem to be newand unique to reinforcement learning.

The origins of temporal-di↵erence learning are in part in animal learning psychology,in particular, in the notion of secondary reinforcers. A secondary reinforcer is a stimulusthat has been paired with a primary reinforcer such as food or pain and, as a result, hascome to take on similar reinforcing properties. Minsky (1954) may have been the first torealize that this psychological principle could be important for artificial learning systems.Arthur Samuel (1959) was the first to propose and implement a learning method thatincluded temporal-di↵erence ideas, as part of his celebrated checkers-playing program(Section 16.2).

Samuel made no reference to Minsky’s work or to possible connections to animallearning. His inspiration apparently came from Claude Shannon’s (1950) suggestion thata computer could be programmed to use an evaluation function to play chess, and that itmight be able to improve its play by modifying this function online. (It is possible thatthese ideas of Shannon’s also influenced Bellman, but we know of no evidence for this.)Minsky (1961) extensively discussed Samuel’s work in his “Steps” paper, suggesting theconnection to secondary reinforcement theories, both natural and artificial.

As we have discussed, in the decade following the work of Minsky and Samuel, littlecomputational work was done on trial-and-error learning, and apparently no computationalwork at all was done on temporal-di↵erence learning. In 1972, Klopf brought trial-and-error learning together with an important component of temporal-di↵erence learning.Klopf was interested in principles that would scale to learning in large systems, and thuswas intrigued by notions of local reinforcement, whereby subcomponents of an overalllearning system could reinforce one another. He developed the idea of “generalizedreinforcement,” whereby every component (nominally, every neuron) views all of itsinputs in reinforcement terms: excitatory inputs as rewards and inhibitory inputs aspunishments. This is not the same idea as what we now know as temporal-di↵erencelearning, and in retrospect it is farther from it than was Samuel’s work. On the otherhand, Klopf linked the idea with trial-and-error learning and related it to the massiveempirical database of animal learning psychology.

Sutton (1978a,b,c) developed Klopf’s ideas further, particularly the links to animallearning theories, describing learning rules driven by changes in temporally successivepredictions. He and Barto refined these ideas and developed a psychological model ofclassical conditioning based on temporal-di↵erence learning (Sutton and Barto, 1981a;Barto and Sutton, 1982). There followed several other influential psychological models ofclassical conditioning based on temporal-di↵erence learning (e.g., Klopf, 1988; Moore et al.,

1986; Sutton and Barto, 1987, 1990). Some neuroscience models developed at this timeare well interpreted in terms of temporal-di↵erence learning (Hawkins and Kandel, 1984;Byrne, Gingrich, and Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986;Friston et al., 1994), although in most cases there was no historical connection.

Our early work on temporal-di↵erence learning was strongly influenced by animallearning theories and by Klopf’s work. Relationships to Minsky’s “Steps” paper and toSamuel’s checkers players were recognized only afterward. By 1981, however, we werefully aware of all the prior work mentioned above as part of the temporal-di↵erence andtrial-and-error threads. At this time we developed a method for using temporal-di↵erencelearning combined with trial-and-error learning, known as the actor–critic architecture,and applied this method to Michie and Chambers’s pole-balancing problem (Barto,Sutton, and Anderson, 1983). This method was extensively studied in Sutton’s (1984)PhD dissertation and extended to use backpropagation neural networks in Anderson’s(1986) PhD dissertation. Around this time, Holland (1986) incorporated temporal-di↵erence ideas explicitly into his classifier systems in the form of his bucket-brigadealgorithm. A key step was taken by Sutton (1988) by separating temporal-di↵erencelearning from control, treating it as a general prediction method. That paper alsointroduced the TD( $\lambda$ ) algorithm and proved some of its convergence properties.

As we were finalizing our work on the actor–critic architecture in 1981, we discovereda paper by Ian Witten (1977, 1976a) which appears to be the earliest publication of atemporal-di↵erence learning rule. He proposed the method that we now call tabular TD(0)for use as part of an adaptive controller for solving MDPs. This work was first submittedfor journal publication in 1974 and also appeared in Witten’s 1976 PhD dissertation.Witten’s work was a descendant of Andreae’s early experiments with STeLLA and othertrial-and-error learning systems. Thus, Witten’s 1977 paper spanned both major threadsof reinforcement learning research—trial-and-error learning and optimal control—whilemaking a distinct early contribution to temporal-di↵erence learning.

The temporal-di↵erence and optimal control threads were fully brought togetherin 1989 with Chris Watkins’s development of Q-learning. This work extended andintegrated prior work in all three threads of reinforcement learning research. Paul Werbos(1987) contributed to this integration by arguing for the convergence of trial-and-errorlearning and dynamic programming since 1977. By the time of Watkins’s work there hadbeen tremendous growth in reinforcement learning research, primarily in the machinelearning subfield of artificial intelligence, but also in artificial neural networks and artificialintelligence more broadly. In 1992, the remarkable success of Gerry Tesauro’s backgammonplaying program, TD-Gammon, brought additional attention to the field.

In the time since publication of the first edition of this book, a flourishing subfield ofneuroscience developed that focuses on the relationship between reinforcement learningalgorithms and reinforcement learning in the nervous system. Most responsible for this isan uncanny similarity between the behavior of temporal-di↵erence algorithms and theactivity of dopamine producing neurons in the brain, as pointed out by a number ofresearchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague,Dayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997). Chapter 15provides an introduction to this exciting aspect of reinforcement learning. Other important

contributions made in the recent history of reinforcement learning are too numerous tomention in this brief account; we cite many of these at the end of the individual chaptersin which they arise.

# Bibliographical Remarks

For additional general coverage of reinforcement learning, we refer the reader to thebooks by Szepesv´ari (2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), andSugiyama, Hachiya, and Morimura (2013). Books that take a control or operations researchperspective include those of Si, Barto, Powell, and Wunsch (2004), Powell (2011), Lewisand Liu (2012), and Bertsekas (2012). Cao’s (2009) review places reinforcement learningin the context of other approaches to learning and optimization of stochastic dynamicsystems. Three special issues of the journal Machine Learning focus on reinforcementlearning: Sutton (1992a), Kaelbling (1996), and Singh (2002). Useful surveys are providedby Barto (1995b); Kaelbling, Littman, and Moore (1996); and Keerthi and Ravindran(1997). The volume edited by Weiring and van Otterlo (2012) provides an excellentoverview of recent developments.

1.2 The example of Phil’s breakfast in this chapter was inspired by Agre (1988).

1.5 The temporal-di↵erence method used in the tic-tac-toe example is developed inChapter 6.

